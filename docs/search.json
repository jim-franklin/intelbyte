[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Franklin Aryee",
    "section": "",
    "text": "I am a data scientist and engineer with a background in applied analytics, system design, and industrial data. I hold a Master of Data Science from the University of British Columbia and a Bachelor of Science in Engineering."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Franklin Aryee",
    "section": "Education",
    "text": "Education\nUniversity of British Columbia\nAug 2024 – Jun 2025\nMaster of Data Science\nKwame Nkrumah University of Science and Technology\nOct 2018 – Nov 2022\nBachelor of Science in Engineering"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "Franklin Aryee",
    "section": "Skills",
    "text": "Skills\nPython, SQL, data modeling, machine learning, time series and telemetry data, dashboard design, system architecture, analytics reporting, GitHub, Quarto"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Franklin Aryee",
    "section": "Experience",
    "text": "Experience\nALS GeoAnalytics | Machine Learning Developer (UBC Capstone)\nMay 2025 – Jun 2025\nTrained deep learning models using PyTorch, designed data pipelines, and supported analytics workflows on operational datasets.\nZerone Analytiqs | Data Scientist, Intern\nOctober 2024 - August 2025\nBuilt analytical models and dashboards using Python and SQL to support business and operational decision making.\nEnergy Commission | Data Analyst, Electricity and Natural Gas\nNov 2022 – Oct 2023\nAnalyzed electricity and natural gas market, generation, and consumption datasets to support regulatory reporting, monitoring, and policy analysis."
  },
  {
    "objectID": "index.html#core-functional-requirements",
    "href": "index.html#core-functional-requirements",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "3.1 Core Functional Requirements",
    "text": "3.1 Core Functional Requirements\nThese define the essential capabilities the platform provides to support equipment monitoring, maintenance, and operational decision-making. They are organized by functional area for clarity.\n\n3.1.1 Asset and Equipment Management\n\nSupport multiple plants, production lines, and machines\n\nMaintain a clear hierarchical relationship between plants, lines, and machines\n\n\n\n3.1.2 Telemetry and Data Ingestion\n\nIngest continuous machine telemetry, including temperature, vibration, throughput, and fault codes\n\nAssociate each telemetry record with a machine identifier and timestamp\n\nRetain telemetry data as a complete historical record for analysis, reporting, and audits\n\n\n\n3.1.3 Operator and User Inputs\n\nAllow operators to log shift reports, inspection results, and issue descriptions\n\nRecord manual machine status updates with user attribution\n\nStore operator comments and inspection notes alongside machine and telemetry data\n\n\n\n3.1.4 Maintenance Management\n\nAutomatically create maintenance requests based on configurable thresholds, fault patterns, and operator reported issues\n\nTrack maintenance requests through their lifecycle, including status updates and resolution details\n\nLink maintenance history to machines and triggering events\n\n\n\n3.1.5 Health and Status Monitoring\n\nCalculate a machine health score using recent telemetry signals and fault history\n\nDetermine the current machine status by reconciling telemetry data, operator inputs, and active maintenance work orders\n\nProvide a Current Equipment Status view showing one row per machine using the most recent available data\n\n\n\n3.1.6 Reporting and Dashboards\n\nProvide dashboards for supervisors to monitor equipment health, status, and maintenance activity\n\nSupport filtering by plant, production line, machine, status, and health indicators"
  },
  {
    "objectID": "index.html#operational-constraints-and-quality-considerations",
    "href": "index.html#operational-constraints-and-quality-considerations",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "3.2 Operational Constraints and Quality Considerations",
    "text": "3.2 Operational Constraints and Quality Considerations\nBeyond core functionality, the platform is designed to operate reliably in a production environment where data volume and operational complexity increase over time. It is expected to scale to thousands of machines generating high-frequency telemetry without requiring major redesign or manual intervention.\nOperational views used by supervisors and maintenance teams must remain highly available and reflect the latest known state of each machine in near real-time. To support audits, investigations, and long-term analysis, all telemetry and operational events are preserved as immutable historical records. The platform enforces role based access control to ensure users can only view or perform actions appropriate to their role, protecting sensitive operational data and workflows. System components are loosely coupled so ingestion, processing, analytics, and user facing features can be updated or extended independently, supporting continuous improvement without disrupting ongoing operations."
  },
  {
    "objectID": "index.html#components-and-functions",
    "href": "index.html#components-and-functions",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "4.1 Components and Functions",
    "text": "4.1 Components and Functions\n\n4.1.1 Frontend (Operator and Supervisor Interfaces)\n\nOperator and Supervisor Web App (Power Apps, Copilot): provides a unified interface for operators, supervisors, and maintenance teams to view current equipment status, review machine details, log shift reports and inspections, submit maintenance requests, and review, prioritize, assign, and track maintenance work orders.\nAnalytics Dashboards (Power BI): visual dashboards showing equipment health, status, and maintenance activity by plant, production line, and machine\n\n\n\n\n\n\n\nPower Apps\n\n\n\n\n\n\n\nPower BI\n\n\n\n\n\n\n\nCopilot\n\n\n\n\n\n\n\nAI Builder\n\n\n\n\n\n\n\n4.1.2 Backend Services and APIs (Application and Processing Layer)\n\nOperational API (Azure Functions): manages core business operations including plant and machine metadata, operator inputs, status updates, and maintenance workflows\nTelemetry Ingestion API (Azure IoT Hub): receives sensor data from machines or plant gateways and checks that required information is present\n\nEvent Stream (Azure Event Hubs): temporarily buffers incoming telemetry to allow the system to scale and continue receiving data even during peak loads\n\nStream Processor (Azure Stream Analytics): continuously processes incoming telemetry, determines the current condition of each machine using predefined rules, and keeps the operational view up to date.\nRule Engine (Azure Functions): evaluates business rules against machine state and history to decide when maintenance actions are required, creating work orders and triggering notifications.\n\nHealth Scoring Service (Azure Functions): calculates a simple health score for each machine using recent telemetry and fault history\n\n\n\n\n\n\n\nAzure Functions\n\n\n\n\n\n\n\nAzure IoT Hub\n\n\n\n\n\n\n\nAzure Stream Analytics\n\n\n\n\n\n\n\nAzure App-Services\n\n\n\n\n\n\n\n4.1.3 Data Layer (Data Storage and Access)\n\nOperational Store (Dataverse or Azure SQL Database): stores structured business data including assets, users, operator reports, and maintenance records\n\nTelemetry Store (Azure Data Explorer): stores all raw telemetry data as a long-term historical record for analysis and audits\n\nCurrent State Store (Azure SQL Database): stores the latest status and health indicators for each machine to support fast operational views\n\n\n\n\n\n\n\nDataverse\n\n\n\n\n\n\n\nAzure SQL Database\n\n\n\n\n\n\n\nAzure Data Explorer\n\n\n\n\n\n\n\nAzure Cosmos DB\n\n\n\n\n\n\n\n4.1.4 Integrations (Access and Notifications)\n\nIdentity Provider (Microsoft Entra ID): manages user login, roles, and access permissions\n\nNotification Channel (Power Automate with Teams or Email): sends alerts when maintenance requests are created or critical issues are detected\n\n\n\n\n\n\n\nMicrosoft Entra\n\n\n\n\n\n\n\nPower Automate\n\n\n\n\n\n\n\nTeams\n\n\n\n\n\n\n\nOutlook"
  },
  {
    "objectID": "index.html#data-flow-and-communication",
    "href": "index.html#data-flow-and-communication",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "4.2 Data Flow and Communication",
    "text": "4.2 Data Flow and Communication\nTelemetry and operator inputs follow different paths, but converge in the current equipment state used for operational dashboards.\n\n4.2.1 Telemetry ingestion flow\n\nMachines or plant gateways send telemetry events to the Telemetry Ingestion API\nEvents are validated and written to the Telemetry Store as immutable, append-only records\nA stream processor consumes telemetry events and updates the Current State Store for the affected machine\nThe Health Scoring Service recalculates machine health scores based on new data or on a defined schedule\nThe Rule Engine evaluates thresholds and fault patterns and creates maintenance requests when conditions are met\n\n\n\n4.2.2 Operator input flow\n\nOperators submit shift logs, inspections, comments, or manual status updates through the web application\nThe Operational API records these inputs in the Operational Store with user attribution\nInputs that affect machine status are reflected in the Current State Store\nOperator-reported issues may directly trigger maintenance requests or contribute to rule evaluation\n\n\n\n4.2.3 Operational dashboard flow\n\nThe Current Equipment Status screen queries the Current State Store to retrieve one row per machine for fast operational monitoring.\nHistorical analysis and audits query the Telemetry Store and join with operational data as needed for reporting and compliance."
  },
  {
    "objectID": "index.html#architecture-pattern-and-rationale",
    "href": "index.html#architecture-pattern-and-rationale",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "4.3 Architecture Pattern and Rationale",
    "text": "4.3 Architecture Pattern and Rationale\nThe architecture follows an event-driven pattern with a derived current-state view to handle high-frequency telemetry that grows rapidly over time. Telemetry is stored as immutable events to preserve a complete historical record for analysis and audits, while a separate current equipment state is maintained to support fast operational queries without scanning large historical datasets.\nBy decoupling telemetry ingestion from operational workflows, the system improves scalability and reliability. Telemetry capture can continue even if downstream processing is delayed, and user-facing views can rely on a lightweight current-state store for near real-time visibility. This design also supports incremental evolution, allowing more advanced analytics and predictive maintenance features to be added without restructuring the core architecture."
  },
  {
    "objectID": "index.html#data-model-overview",
    "href": "index.html#data-model-overview",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "5.1 Data Model Overview",
    "text": "5.1 Data Model Overview\nThe data model is built around three ideas.\nFirst, the platform uses a clear asset hierarchy so machines can always be understood in context. Every machine belongs to a production line, and every production line belongs to a plant. This hierarchy enables filtering, reporting, and access control by site and line.\nSecond, telemetry and events are immutable. Sensor readings and machine signals are written once and never changed. This preserves data integrity and makes the system reliable for trend analysis, root cause investigation, and compliance.\nThird, the system maintains a derived current state for each machine. Instead of recalculating status from millions of telemetry records, the platform keeps one up-to-date row per machine that reflects its latest condition. This is what operational screens and supervisors rely on."
  },
  {
    "objectID": "index.html#entity-relationship-diagram",
    "href": "index.html#entity-relationship-diagram",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "5.2 Entity Relationship Diagram",
    "text": "5.2 Entity Relationship Diagram\nThe Entity Relationship Diagram below shows the core entities such as assets, telemetry, human inputs, and maintenance workflows are connected while keeping responsibilities clearly separated. See Table 1 for field level details.\n\n\n\n\n\n\n\nerDiagram\n  Plant ||--o{ ProductionLine : contains\n  ProductionLine ||--o{ Machine : contains\n  MachineType ||--o{ Machine : classifies\n\n  Machine ||--o{ TelemetryEvent : produces\n  FaultCode ||--o{ TelemetryEvent : appears_in\n\n  Machine ||--o{ OperatorReport : has\n  User ||--o{ OperatorReport : submits\n\n  Machine ||--o{ WorkOrder : has\n  User ||--o{ WorkOrder : creates\n  User ||--o{ WorkOrder : assigned_to\n\n  ThresholdRule }o--o{ WorkOrder : triggers\n\n  Machine ||--|| CurrentMachineState : has_latest\n  TelemetryEvent }o--|| CurrentMachineState : last_telemetry\n  OperatorReport }o--|| CurrentMachineState : last_operator_report\n\n\n\n\nFigure 2: High level ERD for assets, telemetry, operator inputs, and maintenance"
  },
  {
    "objectID": "index.html#entities-and-their-functions",
    "href": "index.html#entities-and-their-functions",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "5.3 Entities and their Functions",
    "text": "5.3 Entities and their Functions\nThe platform relies on a small but complete set of entities.\n\nPlants, production lines, machines, and machine types define the physical structure of the factory.\nTelemetry events capture raw machine signals over time.\nOperator reports capture human observations, inspections, and manual overrides.\nWork orders represent maintenance actions from creation through closure.\nFault codes and threshold rules standardize how issues are detected and interpreted.\nCurrent machine state provides a fast operational snapshot for each machine.\n\nTogether, these entities allow the system to explain what happened, why it happened, and what action was taken."
  },
  {
    "objectID": "index.html#field-level-details",
    "href": "index.html#field-level-details",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "5.4 Field Level Details",
    "text": "5.4 Field Level Details\nThe table below lists the proposed tables with their primary keys, foreign keys, and essential fields required for the MVP.\n\n\n\n\n\nTable 1: Field level details for the MVP schema\n\n\n\n\n\n\n\n\n\n\n\nTable\nPrimary Key\nRelated References\nKey Fields\n\n\n\n\nPlant\nplantId\n\nplantCode, plantName, region, timezone, isActive, createdAt\n\n\nProductionLine\nlineId\nplantId (Plant.plantId)\nlineCode, lineName, area, isActive, createdAt\n\n\nMachineType\nmachineTypeId\n\ntypeCode, typeName, manufacturer, model, createdAt\n\n\nMachine\nmachineId\nlineId (ProductionLine.lineId), machineTypeId (MachineType.machineTypeId)\nmachineCode, machineName, serialNumber, installDate, isActive, createdAt\n\n\nFaultCode\nfaultCodeId\n\nfaultCode, faultName, severity, description\n\n\nTelemetryEvent\ntelemetryEventId\nmachineId (Machine.machineId), faultCodeId (FaultCode.faultCodeId)\neventTimestamp, ingestedAt, temperature, vibration, throughput, statusRaw, payloadJson\n\n\nUser\nuserId\n\ndisplayName, email, role, isActive, createdAt\n\n\nOperatorReport\noperatorReportId\nmachineId (Machine.machineId), userId (User.userId)\nreportTimestamp, reportType, statusOverride, comment\n\n\nWorkOrder\nworkOrderId\nmachineId (Machine.machineId), createdByUserId (User.userId), assignedToUserId (User.userId)\nworkOrderNumber, priority, status, createdAt, closedAt\n\n\nThresholdRule\nthresholdRuleId\nmachineTypeId (MachineType.machineTypeId)\nruleName, metricName, operator, thresholdValue, windowMinutes, severity, isActive\n\n\nCurrentMachineState\nmachineId\nmachineId (Machine.machineId), lastTelemetryEventId (TelemetryEvent.telemetryEventId), lastOperatorReportId (OperatorReport.operatorReportId)\nresolvedStatus, healthScore, lastUpdateAt, openWorkOrderCount"
  },
  {
    "objectID": "index.html#telemetry-and-current-machine-state-strategy",
    "href": "index.html#telemetry-and-current-machine-state-strategy",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "5.5 Telemetry and Current Machine State Strategy",
    "text": "5.5 Telemetry and Current Machine State Strategy\nTelemetry data is modeled as an immutable event stream. Each new reading is inserted and never updated. This guarantees a complete and trustworthy history that can support audits, analytics, and future use cases without redesign.\nOperational views do not query this history directly. Instead, the CurrentMachineState table is continuously updated as telemetry arrives, operator reports are submitted, and work orders change status. Each row represents the latest resolved view of a machine, including status, health score, and maintenance indicators.\nThis separation between append only history and derived operational state is the key scaling decision in the design. It keeps dashboards fast and predictable while preserving full historical detail for deeper analysis and long-term insight."
  },
  {
    "objectID": "index.html#machine-health-scoring",
    "href": "index.html#machine-health-scoring",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "6.1 Machine Health Scoring",
    "text": "6.1 Machine Health Scoring\nThe Machine Health Scoring algorithm converts raw machine signals into a single, easy-to-understand indicator that reflects how healthy a machine is at any moment. The goal is not to predict failure perfectly in the MVP, but to provide a clear, consistent signal that helps supervisors and maintenance teams prioritize attention.\nIndustry platforms often normalize machine health score to a 0–100 scale to simplify interpretation for operations and maintenance teams (Uptime (2022)). The score combines three ideas that are common in industrial monitoring systems:\n\nNormal operation keeps the score high\n\nAbnormal sensor readings reduce the score gradually\n\nFaults and maintenance states reduce the score immediately and visibly\n\nThis balance ensures the score is stable enough for dashboards while still reacting quickly to real issues.\n\n6.1.1 How Often the Score is Recalculated\nThe health score is recalculated each time new telemetry is processed for a machine. We assume telemetry arrives about every 60 seconds, so a 15-minute rolling window provides about 15 samples per metric. Hence, This window reduces short lived noise while still reacting to sustained changes in machine condition.\nRather than recomputing the score from full historical data, the system updates the score incrementally using the previously stored value. In practice, this is done using an Exponentially Weighted Moving Average (EWMA), which balances responsiveness with noise reduction in streaming telemetry (Montgomery (2020)).\nIf telemetry is missing for too long, the previous score is no longer reliable. As a safeguard, when the time since the last telemetry event exceeds twice the rolling window (in this case, more than 30 minutes for the 15-minute window), the smoothing state is reset and the score is reinitialized from the current raw telemetry based score. See Section 6.1.4 for the health scoring pseudocode.\n\n\n\n\n\n\nThe machines are assumed to publish telemetry approximately every 60 seconds. A 15-minute rolling window provides 15 samples per metric. The health score is recalculated whenever new telemetry arrives and is typically updated once per minute.\n\n\n\n\n\n6.1.2 Where the Score is Stored\nHealth scoring is performed by the Health Scoring Service (Azure Functions), which computes the score as telemetry arrives (approximately every minute). The score is stored in the CurrentMachineState table, which represents the latest operational view for each machine:\n\nTable: CurrentMachineState\n\nPrimary key: machineId\n\nRelated references:\n\nmachineId -&gt; Machine.machineId\n\nlastTelemetryEventId -&gt; TelemetryEvent.telemetryEventId\n\nlastOperatorReportId -&gt; OperatorReport.operatorReportId\n\n\nThe scoring service keeps health scoring independent from status resolution and maintenance workflows by updating only the fields it owns:\n\nhealthScore\n\nlastUpdateAt\n\n\n\n6.1.3 Scoring Logic\nAt each update, the score is computed using recent machine behavior, not the full telemetry history. The scoring logic combines:\n\nThe latest telemetry readings, aggregated as rolling averages over a 15-minute window\n\nCurrent machine status (Running, Idle, Fault, UnderMaintenance)\n\nAny active fault codes and their severity (High, Medium)\n\nOperator flags from inspections (IssueObserved, MinorConcern)\n\nThe pseudocode in Section 6.1.4 shows how each signal contributes to the final score and how EWMA smoothing stabilizes the result for operational dashboards.\n\n\n6.1.4 Pseudocode for Scoring\ndef compute_health_score(previous_score, minutes_since_last_telemetry):\n1    rawScore = 100\n2    smoothing_param = 0.2\n\n    # sensor penalties (computed from last 15 minutes of data)\n3    rawScore -= temperature_penalty\n    rawScore -= vibration_penalty\n    rawScore -= throughput_penalty\n\n    # fault severity penalties\n4    if fault_severity == \"High\":\n        rawScore -= 60\n    elif fault_severity == \"Medium\":\n        rawScore -= 30\n\n    # apply operator input only if a report exists\n5    if operator_flag == \"IssueObserved\":\n        rawScore -= 20\n    elif operator_flag == \"MinorConcern\":\n        rawScore -= 10\n\n    # maintenance state cap\n6    if status == \"UnderMaintenance\":\n        rawScore = min(rawScore, 60)\n\n    # initialization / reset rule (no previous score or stale telemetry)\n7    if previous_score is None or minutes_since_last_telemetry &gt; 30:\n        healthScore = rawScore\n    else:\n        # smoothing (EWMA)\n8        healthScore = (\n            smoothing_param * rawScore +\n            (1 - smoothing_param) * previous_score\n        )\n\n    # ensure the final score stays between 0 (worst) and 100 (best)\n9    healthScore = max(0, min(100, healthScore))\n\n    return healthScore\n\n1\n\nStart from a baseline score and compute a fresh rawScore for this update.\n\n2\n\nSet the EWMA weight so recent data influences the score without causing jumps.\n\n3\n\nApply penalties based on 15-minute rolling window sensor summaries\n\n4\n\nApply a strong penalty when an active fault is detected, scaled by severity.\n\n5\n\nReduce the score when operators flag issues in inspections or shift reports.\n\n6\n\nCap the score during maintenance to reflect reduced availability.\n\n7\n\nIf there is no previous score (MVP start) or telemetry is stale (&gt;2× rolling window), reinitialize from rawScore.\n\n8\n\nOtherwise smooth using EWMA to reduce noise while keeping responsiveness.\n\n9\n\nClamp the final score to stay between 0 and 100."
  },
  {
    "objectID": "index.html#latest-machine-status-resolution",
    "href": "index.html#latest-machine-status-resolution",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "6.2 Latest Machine Status Resolution",
    "text": "6.2 Latest Machine Status Resolution\nThe platform resolves CurrentMachineState.resolvedStatus by evaluating multiple signal sources in a fixed order of precedence. This ensures the resulting status is predictable, explainable, and aligned with real operational decision-making.\n\n6.2.1 Status Precedence\nStatus precedence ensures maintenance and safety decisions take priority, while allowing the system to automatically return to telemetry driven status when conditions normalize. It is applied as follows (highest wins):\n\nUnderMaintenance\nIf one or more maintenance work orders are active for a machine, the machine is considered UnderMaintenance regardless of telemetry or operator inputs. This condition is represented by openWorkOrderCount &gt; 0, derived from WorkOrder.status.\nManual operator override\nIf an operator submits a report containing statusOverride, the most recent override is applied while it is still valid. This allows short term safety or inspection decisions to override automated signals.\nTelemetry derived status\nWhen neither maintenance nor a manual override is active, the status is derived from the latest telemetry event. If faultCodeId is present the status is Fault; otherwise the status falls back to the device reported operating state stored in TelemetryEvent.statusRaw (for example Running or Idle).\n\n\n\n\n\n\n\nWorkOrder.status drives openWorkOrderCount. Any non-zero count forces CurrentMachineState.resolvedStatus = 'UnderMaintenance' until all active work orders are closed and openWorkOrderCount returns to 0.\n\n\n\n\n\n\n\n\n\nOperator overrides are intentionally time boxed (example: 4 hours). After the window expires, status resolution automatically falls back to telemetry without requiring manual cleanup.\n\n\n\n\n\n6.2.2 Initialization at Launch\nThis runs once to create the CurrentMachineState at MVP launch. After this, the system stays current via event driven upserts (Section 6.2.4).\n-- Physical table (read model)\nCREATE TABLE IF NOT EXISTS CurrentMachineState (\n  machineId               BIGINT PRIMARY KEY,\n  resolvedStatus          VARCHAR(32) NOT NULL,\n  healthScore             NUMERIC(5,2) NULL,\n  openWorkOrderCount      INT NOT NULL DEFAULT 0,\n  lastTelemetryEventId    BIGINT NULL,\n  lastOperatorReportId    BIGINT NULL,\n  lastUpdateAt            TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n);\n\n\n6.2.3 Event Driven Updates\nAfter the initialization, the platforms performs small targeted updates:\n\nWhen telemetry arrives for a machine, update that machine’s row\n\nWhen a work order opens or closes for a machine, update that machine’s row\n\nWhen an operator submits an override for a machine, update that machine’s row\n\nThe query below shows per machine update executed when telemetry, work order or operator override is received for a specific machine.\n\n\n\n\n\n\nNote\n\n\n\nThe SQL code block is executed with parameters supplied by the Stream Processor. These parameters are taken directly from the incoming telemetry message from Telemetry Ingestion API. They include :machineId, :telemetryEventId, :eventTimestamp, :statusRaw, :faultCodeId.\n\n\n-- Per machine event update (event driven upsert)\n1WITH WorkOrderSignals AS (\n  SELECT\n    COUNT(*) FILTER (WHERE status IN ('Open','InProgress')) AS openWorkOrderCount,\n    MAX(COALESCE(closedAt, createdAt)) AS lastWorkOrderChangeAt\n  FROM WorkOrder\n  WHERE machineId = :machineId\n),\n\n2LatestOperatorOverride AS (\n  SELECT\n    operatorReportId,\n    reportTimestamp,\n    statusOverride\n  FROM OperatorReport\n  WHERE machineId = :machineId\n    AND statusOverride IS NOT NULL\n  ORDER BY reportTimestamp DESC\n  LIMIT 1\n)\n\n3INSERT INTO CurrentMachineState (\n  machineId,\n  resolvedStatus,\n  openWorkOrderCount,\n  lastTelemetryEventId,\n  lastOperatorReportId,\n  lastUpdateAt\n)\n\nSELECT\n  :machineId,\n\n4  CASE\n    WHEN COALESCE(wos.openWorkOrderCount, 0) &gt; 0 THEN 'UnderMaintenance'\n\n    WHEN loo.statusOverride IS NOT NULL\n      AND loo.reportTimestamp &gt;= CURRENT_TIMESTAMP - INTERVAL '4 hours'\n      THEN loo.statusOverride\n\n    WHEN :faultCodeId IS NOT NULL THEN 'Fault'\n    ELSE COALESCE(:statusRaw, 'Idle')\n  END AS resolvedStatus,\n\n  COALESCE(wos.openWorkOrderCount, 0) AS openWorkOrderCount,\n  :telemetryEventId AS lastTelemetryEventId,\n  loo.operatorReportId AS lastOperatorReportId,\n\n5  GREATEST(\n    COALESCE(:eventTimestamp, TIMESTAMP '1970-01-01'),\n    COALESCE(loo.reportTimestamp, TIMESTAMP '1970-01-01'),\n    COALESCE(wos.lastWorkOrderChangeAt, TIMESTAMP '1970-01-01')\n  ) AS lastUpdateAt\nFROM WorkOrderSignals wos\nLEFT JOIN LatestOperatorOverride loo ON TRUE\n\n6ON CONFLICT (machineId) DO UPDATE\nSET\n  resolvedStatus = EXCLUDED.resolvedStatus,\n  openWorkOrderCount = EXCLUDED.openWorkOrderCount,\n  lastTelemetryEventId = EXCLUDED.lastTelemetryEventId,\n  lastOperatorReportId = EXCLUDED.lastOperatorReportId,\n  lastUpdateAt = EXCLUDED.lastUpdateAt;\n\n1\n\nWorkOrderSignals - Counts active work orders and captures the latest maintenance change for the machine.\n\n2\n\nLatestOperatorOverride – Fetches the most recent valid operator status override for the machine.\n\n3\n\nInsert into CurrentMachineState – Writes or updates the single current state row for the machine.\n\n4\n\nStatus resolution logic – Determines the machine’s status using maintenance, operator override, then telemetry.\n\n5\n\nlastUpdateAt calculation – Records the most recent timestamp from telemetry, maintenance, or operator input.\n\n6\n\nUpsert logic – Guarantees exactly one up-to-date row per machine in CurrentMachineState.\n\n\n\n\n6.2.4 How Health Score fits into the current view\nOnce a new health score in Section 6.1.4 is computed, the Health Scoring Service automatically updates the current state for the affected machine using the following update statement.\nUPDATE CurrentMachineState\nSET\n1  healthScore = new_score_value,\n2  lastUpdateAt = CURRENT_TIMESTAMP\n3WHERE machineId = target_machine_id;\n\n1\n\nnew_score_value is the computed health score produce by the scoring function for each machine\n\n2\n\nlastUpdateAt is bumped so the dashboard can tell the record was updated recently\n\n3\n\ntarget_machine_id identifies which machine row to update\n\n\n\n\n\n\n\n\nCurrentMachineState.lastUpdateAt represents the most recent time any part of the machine’s current state changed. This includes:\n\na status change driven by telemetry, work orders, or operator overrides\nan update to openWorkOrderCount\na new healthScore written by the Health Scoring Service\n\nBecause both status resolution and health scoring update the same read model, lastUpdateAt reflects overall state freshness, not just telemetry arrival time."
  },
  {
    "objectID": "index.html#current-equipment-status-screen",
    "href": "index.html#current-equipment-status-screen",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "7.1 Current Equipment Status Screen",
    "text": "7.1 Current Equipment Status Screen\nThe Current Equipment Status screen is the primary operator view. It answers one question quickly: what is happening right now across the line.\nWhat it shows\n\nFleet level counts by resolvedStatus (Running, Idle, Fault, UnderMaintenance)\n\nA sortable table of machines with resolvedStatus, healthScore, openWorkOrderCount, and lastUpdateAt\n\nQuick filters (Plant, Production line, Machine type)"
  },
  {
    "objectID": "index.html#operational-and-maintenance-views",
    "href": "index.html#operational-and-maintenance-views",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "7.2 Operational and Maintenance Views",
    "text": "7.2 Operational and Maintenance Views\nThe Operational and Maintenance views support supervisors and maintenance teams. They focus on exceptions and action queues rather than the full fleet.\nWhat it shows\n\nMachines currently in Fault with the latest fault context\n\nMachines currently UnderMaintenance with openWorkOrderCount\n\nA small “Recently changed” list using lastUpdateAt\n\nA drill down entry point using lastTelemetryEventId and lastOperatorReportId"
  },
  {
    "objectID": "index.html#key-security-concerns",
    "href": "index.html#key-security-concerns",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "8.1 Key Security Concerns",
    "text": "8.1 Key Security Concerns\nThe primary security concerns addressed by the platform include:\n\nUnauthorized access to operational data, such as machine health, fault history, and maintenance records\n\nImproper modification of machine state, which could lead to unsafe operational decisions\n\nLoss of traceability, where it becomes unclear who changed a status or triggered maintenance\n\nExposure of sensitive telemetry or operational metadata across plants and regions\n\nThe architecture mitigates these risks by separating ingestion, processing, and presentation layers, and by enforcing access controls at every boundary."
  },
  {
    "objectID": "index.html#authentication-and-authorization",
    "href": "index.html#authentication-and-authorization",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "8.2 Authentication and Authorization",
    "text": "8.2 Authentication and Authorization\nAuthentication and authorization are handled centrally using Microsoft Entra ID, ensuring consistent identity management across all user interfaces and services. APIs validate access tokens on every request, ensuring users can only view or modify data permitted by their role.\nAuthentication\nAll users access the platform through Entra ID using enterprise credentials. This supports single sign-on, multi-factor authentication, and centralized user lifecycle management.\nAuthorization\nRole-based access control (RBAC) governs what actions users can perform. Typical roles include:\n\nOperator: view machine status, submit operator reports\n\nSupervisor: review machine health, approve or prioritize work orders\n\nMaintenance team: manage work orders and maintenance actions\n\nAdministrator: manage users, roles, and system configuration"
  },
  {
    "objectID": "index.html#data-integrity-and-auditability",
    "href": "index.html#data-integrity-and-auditability",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "8.3 Data Integrity and Auditability",
    "text": "8.3 Data Integrity and Auditability\nTrust in machine condition and maintenance decisions is critical for safe and reliable operations. This design makes it easy to review incidents after they occur, meet regulatory requirements, and continuously improve how operational rules are applied.\nThe platform ensures data integrity and auditability through:\n\nAppend-only telemetry storage, preserving all raw sensor data for analysis and audits\nExplicit pointers in the current state, such as lastTelemetryEventId and lastOperatorReportId, which link the current view back to its source records\nClear separation of concerns, where:\n\nStatus resolution is handled by event processing and workflows\nHealth scoring is computed independently by a scoring service\n\nTimestamps on all updates, allowing operators and auditors to understand when and why a state changed"
  },
  {
    "objectID": "index.html#data-growth-and-performance",
    "href": "index.html#data-growth-and-performance",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "9.1 Data Growth and Performance",
    "text": "9.1 Data Growth and Performance\nTelemetry data grows rapidly as machines publish frequent updates. The platform approach ensures predictable performance as the number of machines, plants, and telemetry volume increases.The architecture addresses this by:\n\nSeparating historical and operational data\n\nRaw telemetry is stored in a scalable analytics store optimized for high write volumes\nThe CurrentMachineState table stores only one row per machine for fast reads\n\nEvent-driven processing\n\nTelemetry events update only the affected machine’s current state, avoiding periodic full-table scans or fleet-wide recomputation\n\nRead-optimized dashboards\n\nDashboards query the CurrentMachineState table directly, enabling fast refreshes even as historical data grows into millions or billions of records."
  },
  {
    "objectID": "index.html#failure-scenarios-and-resilience",
    "href": "index.html#failure-scenarios-and-resilience",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "9.2 Failure Scenarios and Resilience",
    "text": "9.2 Failure Scenarios and Resilience\nThe system is designed to remain operational under common failure scenarios:\n\nTelemetry spikes or bursts: Event buffering absorbs temporary surges without dropping data.\n\nTemporary processing delays: If downstream services slow down, telemetry ingestion continues and processing resumes when capacity is available.\n\nService restarts or redeployments: Stateless processing components can restart safely, using stored state and telemetry history to resume operation.\n\nPartial data loss or gaps: Rolling windows and incremental scoring ensure that short interruptions do not permanently distort machine health indicators."
  },
  {
    "objectID": "index.html#future-enhancements",
    "href": "index.html#future-enhancements",
    "title": "Designing a Scalable Factory Equipment Health and Maintenance Platform",
    "section": "10.1 Future Enhancements",
    "text": "10.1 Future Enhancements\nAs the platform evolves beyond the MVP, the following enhancements can be considered:\n\nIntroduce predictive maintenance models using historical telemetry and work order outcomes\n\nRefine health scoring using machine-type-specific models and adaptive thresholds\n\nAdd automated anomaly detection to complement rule-based logic\n\nEnhance dashboards with trend views and cross-plant comparisons\n\nOverall, the architecture provides a solid foundation that balances simplicity for the MVP with a clear path toward more advanced industrial analytics and automation."
  }
]